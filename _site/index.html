<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Jinghui Chen | Home</title>
  <meta name="description" content="Homepage of Jinghui Chen">
  <meta name="author" content="Jinghui Chen">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=/libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=/libs/custom/my_css.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=/libs/external/jquery-3.1.1.min.js></script>


  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=/libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=/libs/external/github-prettify-theme.css>-->
  <script src=/libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.jpeg>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.jpeg>

  <!-- Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164072186-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-164072186-1');
  </script>
  

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">

    <section class="header">
      <div class="row">
        <div class="three columns">
          <a href="/"><img title="Photo" class="u-max-full-width" src='/assets/profile-pics/pic_new.jpg'></a>
        </div>
        <div class="nine columns main-description">
            <h1>Jinghui Chen</h1>
            <!-- <p>(he/him/his)</p> -->
            <p>Assistant Professor, Ph.D.</p>
            <p>Penn State University</p>
            <p>Email: jzc5917 [at] psu [dot] edu</p>
            <p>

              <span onclick="window.open('https://scholar.google.com/citations?user=mKia7Y4AAAAJ&hl=en')" style="cursor: pointer">
                <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://twitter.com/netfox001')" style="cursor: pointer">
                <i class="fa fa-twitter" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://www.linkedin.com/in/jinghui-chen-53a2a692')" style="cursor: pointer">
                <i class="fa fa-linkedin-square" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://github.com/jinghuichen')" style="cursor: pointer">
                <i class="fa fa-github" aria-hidden="true"></i>
              </span>

              
              
            </p>
        </div>
      </div>
    </section>

    <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href=/index.html#about>About</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#news>News</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#publications>Publications</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#research>Research</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#students>Students</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#teaching>Teaching</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#service>Service</a></li>
          <!-- <li class="navbar-item"><a class="navbar-link" href=/index.html#resume>Vita</a></li> -->
        </ul>
      </div>
    </nav>

    <script>
function unhide(divID) {
  console.log(divID)
  var item = document.getElementById(divID);
  console.log('hi')
  if (item){
    item.className=(item.className=='hidden')?'unhidden':'hidden';
  }
}
</script>

<!-- ========== ABOUT ========== -->

<div class="docs-section">
  <span class="anchor" id="about"></span>
  <h4>About</h4>

  <p>
  I am an Assistant Professor in the <a href="https://ist.psu.edu/" target="_blank">College of Information Sciences and Technology</a> at Penn State University</a>. I received my Ph.D. in the Department of Computer Science, University of California, Los Angeles (UCLA) working with <a href="http://web.cs.ucla.edu/~qgu/" target="_blank">Prof. Quanquan Gu </a> in 2021. I received my B.E. in the Department of Electrical Engineering and Information Science at the University of Science and Technology of China in 2015.
  </p>



  <p style="color:#a70c0c;"><b>Prospective Students</b>: I’m looking for highly motivated PhD/intern students to join my group. The official PhD application deadline for the Fall 2024 application cycle is Dec 15, 2023 (<a href="https://ist.psu.edu/prospective/graduate/application/phd" target="_blank">details</a>). 
  <b>If you’re interested in joining my lab, please fill and see instructions in the following <a href="https://forms.office.com/r/xqPF6xmySq" target="_blank"> form</a></b> (feel free to skip optional questions). 

         <!-- See more info <a href="https://www.1point3acres.com/bbs/thread-792187-1-1.html" target="_blank"> here</a>. -->
      <!-- I will go over your applications and contact you if you fit our lab. You don’t have to send me another email, as the form will generate one automatically.  -->
      <!-- (Note that although we may not be able to respond to each individual message due to the large number of messages we receive, we do review all applications.) -->
  </p>


  <p> <strong style="color:#a70c0c;">Research Interests:</strong> 
  My research interests broadly include the theory and applications in different aspects of machine learning, with particular interests on building <strong style="color:#a70c0c;">efficient</strong> and <strong style="color:#a70c0c;">trustworthy</strong> machine learning models. Recently, we are particularly interested in the following research topics:
  <ul style="margin-left: 40px">
    <li>Trustworthiness and safety issues in Large Language Models (LLM alignments, LLM robustness, etc.)</li>
    <li>Security and privacy issues for other emerging machine learning models (multimodal foundation models, federated learning, diffusion models, etc.)</li>
    <li>Efficient optimization strategies for training large scale foundataion models/federated learning (adaptive gradient optimizers, parameter-efficient training, etc.)</li>

  </ul> 
 
  </p>  


</div>
 

<!-- ========== NEWS ========== -->
<!-- Had to add empty anchor to solve the problem of fixed header
     hiding part of paper
-->
<span class="anchor" id="news"></span>
<div class="docs-section">
  <h4>News</h4>
  <!-- <p> Scroll for older news </p> -->
  <UL style="list-style-type: none; padding: 2px; overflow-y: scroll; height:500px;">
    <li>
      [12/2023] One paper is accepted to AAAI 2024!  
    </li> 
    <li>
      [12/2023] Two papers are accepted to USENIX 2024!  
    </li> 
    <li>
      [09/2023] Five papers are accepted to NeurIPS 2023!  
    </li> 
    <li>
      [08/2023] We are organizing the <a href="https://federated-learning.org/fl@fm-neurips-2023/" target="_blank">Workshop on Federated Learning in the Age of Foundation Models</a> at NeurIPS 2023 (FL@FM-NeurIPS’23). Submissions are welcome! 
    </li> 
    <li>
      [08/2023] Our paper is accepted to CIKM 2023: "RoCourseNet: Robust Training of a Prediction Aware Recourse Model" 
    </li> 
    <li>
      [05/2023] Our paper is accepted to KDD 2023: "PAT: Geometry-Aware Hard-Label Black-Box Adversarial Attacks on Text" 
    </li> 
    <li>
      [05/2023] Our paper is accepted to UAI 2023: "Benign Overfitting in Adversarially Robust Linear Classification" 
    </li> 
    <li>
      [04/2023] Our paper is accepted to ICML 2023: "Graph Contrastive Backdoor Attacks" 
    </li> 
    <li>
      [01/2023] Our paper is accepted to WWW 2023: "Do Language Models Plagiarize?" 
    </li> 
    <li>
      [01/2023] Our paper is accepted to ICLR 2023: "Spectral Augmentation for Self-Supervised Learning on Graphs" 
    </li> 
    <li>
      [12/2022] Our paper is accepted to AAAI 2023: "On the Vulnerability of Backdoor Defenses for Federated Learning" 
    </li> 
    <li>
      [09/2022] Our paper is accepted to NeurIPS 2022: "One-shot Neural Backdoor Erasing via Adversarial Weight Masking" 
    </li> 
    <li>
      [05/2022] Our paper is accepted to KDD 2022: "LeapAttack: Hard-Label Adversarial Attack on Text via Gradient-Based Optimization" 
    </li> 
    <li>
      [05/2022] Our paper is accepted to ICML 2022: "Communication-Efficient Adaptive Federated Learning" 
    </li>  
    <li>
      [05/2022] Dr. Chen recieved received Cisco Faculty Research Award!
    </li>   
    <li>
      [01/2022] Our paper is accepted to ICLR 2022: "Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations" 
    </li>    
    <li>
      [01/2022] Our paper is accepted to AISTATS 2022: "Communication-Compressed Adaptive Gradient Method for Distributed Nonconvex Optimization" 
    </li>        
    <li>
      [12/2021] Our paper is accepted to AAAI 2022: "Efficient Robust Training via Backward Smoothing" 
    </li>  
    <li>
      [09/2021] Our paper is accepted to NeurIPS 2021:
       "Do Wider Neural Networks Really Help Adversarial Robustness?"
    </li>  
    <li>
      [06/2021] I recieved UCLA Outstanding Graduate Student Research Award. 
    </li> 
    <li>
      [04/2021] I will join the College of Information Sciences and Technology (IST) at Penn State University (PSU) in Fall 2021 as a tenure-track assistant professor. 
    </li> 
    <li>
      [07/2020] Released  <a href="https://github.com/uclaml/RayS" target="_blank">Model Robustness (ADBD) Leaderboard </a>under RayS attack: 
       Benchmarking state-of-the-art robust trained models with ADBD metric
    </li> 
    <li>
      [05/2020] Our paper is accepted to KDD 2020: 
       "RayS: A Ray Searching Method for Hard-label Adversarial Attack"
    </li> 
    <li>
      [04/2020] Our paper is accepted to IJCAI 2020:
       "Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks"
    </li> 
    <li>
      [04/2020] We just launched a project using machine learning and AI to combat Covid-19!
       <a href="https://covid19.uclaml.org/" target="_blank"> Live data visualization and new cases / peak predictions </a>
    </li> 
    <li>
      [01/2020] Our paper is accepted to AISTATS 2020:
       "Understanding the Intrinsic Robustness of Image Distributions using Conditional Generative Models"
    </li> 
    <li>
      [11/2019] Our paper is accepted to AAAI 2020:
       "A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks" 
    </li> 
    
     
    
  </UL>
</div>




 



<!-- ========== PUBLICATIONS ========== -->
<div class="docs-section" id="publications">
  <h4>Publications</h4>

  <p>Full publications on <a href="https://scholar.google.com/citations?user=mKia7Y4AAAAJ&hl=en" target="_blank">Google Scholar</a>.
  <br> <sup><b>E</b></sup> indicates authors with equal contribution. <u>underline</u> indicates students supervised. </p> 

  <ul class="tab-nav">
    <li><div class="button active" data-ref="#all">All</div></li>
    <li><div class="button" data-ref="#at">At PSU</div></li>    
  </ul>


  <div class="tab-content">
 
 
    <ol class="tab-pane active" id="all">

      <li class="paper">
        <b>VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models</b> 
        <br>
        <p>Ziyi Yin, Muchao Ye, <u>Tianrong Zhang</u>, Jiaqi Wang, Han Liu, <b>Jinghui Chen</b>, Ting Wang and Fenglong Ma, <i>in Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence (<strong style="color:red;">AAAI</strong>), Vancouver, Canada, 2024. </i>
        <!-- <a href=" " target="_blank">[Paper]</a> -->
        </p>
       </li>

      <li class="paper">
        <b>On Defending Contrastive Learning against Backdoor Attacks</b> 
        <br>
        <p>Changjiang Li, Ren Pang, <u>Bochuan Cao</u>, Zhaohan Xi, <b>Jinghui Chen</b>, Shouling Ji and Ting Wang, <i>in Proceedings of the 33rd USENIX Security Symposium (<strong style="color:red;">USENIX</strong>), Philadelphia, PA, USA, 2024.</i>
        <!-- <a href=" " target="_blank">[Paper]</a> -->
        </p>
       </li>


      <li class="paper">
        <b>Unveiling the Secrets without Data: Can Graph Neural Networks Be Exploited through Data-Free Model Extraction Attacks?</b> 
        <br>
        <p>Yuanxin Zhuang, Chuan Shi, Mengmei Zhang, <b>Jinghui Chen</b>, Lingjuan Lyu, Pan Zhou and Lichao Sun,  <i>in Proceedings of the 33rd USENIX Security Symposium (<strong style="color:red;">USENIX</strong>), Philadelphia, PA, USA, 2024.</i>
        <!-- <a href=" " target="_blank">[Paper]</a> -->
        </p>
       </li>

      <li class="paper">
        <b>Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections</b> 
        <br>
        <p><u>Yuanpu Cao</u>, <u>Bochuan Cao</u> and <b>Jinghui Chen</b>, <i>arXiv:2312.00027, 2023.</i>
        <a href="https://arxiv.org/abs/2312.00027" target="_blank">[Paper]</a>
        </p>
       </li>

      <li class="paper">
        <b>On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?</b> 
        <br>
        <p>Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, <u>Bochuan Cao</u>, Lu Lin, Jinyuan Jia, <b>Jinghui Chen</b> and Dinghao Wu, <i>arXiv:2310.01581, 2023. </i>
        <a href="https://arxiv.org/abs/2310.01581" target="_blank">[Paper]</a>
        </p>
       </li>

       <li class="paper">
        <b>Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM</b> 
        <br>
        <p><u>Bochuan Cao</u><sup><b>E</b></sup>, <u>Yuanpu Cao</u><sup><b>E</b></sup>,  Lu Lin, and <b>Jinghui Chen</b>, <i>arXiv:2309.14348, 2023. </i>
        <a href="https://arxiv.org/abs/2309.14348" target="_blank">[Paper]</a>
        </p>
       </li>

      <li class="paper">
        <b>IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI</b> 
        <br>
        <p><u>Bochuan Cao</u>, Changjiang Li, Ting Wang, Jinyuan Jia, Bo Li and <b>Jinghui Chen</b>, <i>in Proceedings of the 37th Conference on Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, Louisiana, USA, 2023. </i>
        <a href="https://arxiv.org/abs/2310.19248" target="_blank">[Paper]</a>
        </p>
       </li>

      <li class="paper">
        <b>VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models</b><br>
        <p>Ziyi Yin, Muchao Ye, <u>Tianrong Zhang</u>, Tianyu Du, Jinguo Zhu, Han Liu, <b>Jinghui Chen</b>, Ting Wang and Fenglong Ma, <i>in Proceedings of the 37th Conference on Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, Louisiana, USA, 2023. </i>
        <a href="https://arxiv.org/abs/2310.04655" target="_blank">[Paper]</a> 
        </p>
      </li>
 
      <li class="paper">
        <b>A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning</b></br>
        <p>Hangfan Zhang, Jinyuan Jia, <b>Jinghui Chen</b>, Lu Lin and Dinghao Wu, <i>in Proceedings of the 37th Conference on Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, Louisiana, USA, 2023. </i>
        <a href="https://openreview.net/forum?id=S6ajVZy6FA" target="_blank">[Paper]</a> 
        </p>
      </li>

      <li class="paper">
        <b>Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks</b></br>
        <p>Tianyu Du, Zhaohan Xi, Changjiang Li, Ren Pang, Shouling Ji, <b>Jinghui Chen</b>, Fenglong Ma and Ting Wang, <i>in Proceedings of the 37th Conference on Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, Louisiana, USA, 2023. </i>
        <a href="https://arxiv.org/abs/2309.13256" target="_blank">[Paper]</a> 
        </p>
      </li>

      <li class="paper">
        <b>UniT: A Unified Look at Certified Robust Training against Text Adversarial Perturbation</b></br>
        <p>Muchao Ye, Ziyi Yin, <u>Tianrong Zhang</u>, Tianyu Du, <b>Jinghui Chen</b>, Ting Wang and Fenglong Ma, <i>in Proceedings of the 37th Conference on Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, Louisiana, USA, 2023. </i>
        <a href="https://openreview.net/forum?id=oGxE2Nvlda" target="_blank">[Paper]</a> 
        </p>
      </li>

      <li class="paper">
        <b>RoCourseNet: Robust Training of a Prediction Aware Recourse Model</b></br>
        <p>Hangzhi Guo, Feiran Jia, <b>Jinghui Chen</b>, Anna Squicciarini and Amulya Yadav, <i>in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (<strong style="color:red;">CIKM</strong>), Birmingham, UK, 2023. </i>
        <a href="https://arxiv.org/abs/2206.00700" target="_blank">[Paper]</a> 
        </p>
    <!--     <p style="color:blue;">A short version of this paper also appears on ICML 2022 Workshop on Adversarial Machine Learning Frontiers. </p>
           <li class="paper-buttons">
              <a class="button" href="https://arxiv.org/abs/2206.00700" target="_blank">Paper</a>
          </li> -->
      </li>


      <li class="paper">
        <b>PAT: Geometry-Aware Hard-Label Black-Box Adversarial Attacks on Text</b></br>
        <p>Muchao Ye, <b>Jinghui Chen</b>, Chenglin Miao, Han Liu, Ting Wang and Fenglong Ma, <i> in Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<strong style="color:red;">KDD</strong>), San Diego, CA, USA, 2023.</i>
        <a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599461" target="_blank">[Paper]</a>
        </p>           
      </li>

      <li class="paper">
        <b>Benign Overfitting in Adversarially Robust Linear Classification</b></br>
        <p><b>Jinghui Chen<sup><b>E</b></sup></b>, Yuan Cao<sup><b>E</b></sup>, and Quanquan Gu, <i> in Proceedings of the 39th Conference on Uncertainty in Artificial Intelligence (<strong style="color:red;">UAI</strong>), Pittsburgh, PA, USA, 2023.</i> <a href="https://arxiv.org/abs/2112.15250" target="_blank">[Paper]</a> </p>
         <!-- <p style="color:blue;">A short version of this paper also appears on ICML 2021 Workshop on Overparameterization: Pitfalls and Opportunities.</p> -->
      </li>

      <li class="paper">
        <b>Graph Contrastive Backdoor Attacks</b></br>
        <p>Hangfan Zhang, <b>Jinghui Chen</b>, Lu Lin, Jinyuan Jia and Dinghao Wu, <i> in Proceedings of the 40th International Conference on Machine Learning (<strong style="color:red;">ICML</strong>), Hawaii, USA, 2023. </i> 
        <a href="https://proceedings.mlr.press/v202/zhang23e/zhang23e.pdf" target="_blank">[Paper]</a> </p>
      </li>


      <li class="paper">
        <b>Tackling the Data Heterogeneity in Asynchronous Federated Learning with Cached Update Calibration</b></br>
        <p><u>Yujia Wang</u>, <u>Yuanpu Cao</u>, <u>Jingcheng Wu</u>, <u>Ruoyu Chen</u> and <b>Jinghui Chen</b>, <i> in ICML 2023 Workshop on Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities (<strong style="color:red;">FL-ICML</strong>), 2023. </i> 
        <a href="https://openreview.net/forum?id=WpjdIMouJj" target="_blank">[Paper]</a> </p>
      </li>



      <li class="paper">
        <b>Multiple Models for Outbreak Decision Support in the Face of Uncertainty</b></br>
        <p>Katriona Shea, ..., <b>Jinghui Chen</b>, ..., Michael C. Runge., <i>in Proceedings of the National Academy of Sciences (<strong style="color:red;">PNAS</strong>), 2023.</i>
        <a href="https://www.pnas.org/doi/10.1073/pnas.2207537120" target="_blank">[Paper]</a> </p>
        </li>


      <li class="paper">
        <b>Do Language Models Plagiarize?</b></br>
        <p>Lee, Jooyoung, Thai Le, <b>Jinghui Chen</b>, and Dongwon Lee, <i>in Proceedings of the ACM Web Conference (<strong style="color:red;">WWW</strong>), Austin, Texas, USA, 2023. </i>
        <a href="https://arxiv.org/abs/2203.07618" target="_blank">[Paper]</a> </p>    
      </li>


      <li class="paper">
        <b>Spectral Augmentation for Self-Supervised Learning on Graphs</b></br>
        <p>Lu Lin, <b>Jinghui Chen</b>, Hongning Wang, <i> in Proceedings of the 11th International Conference on Learning Representations (<strong style="color:red;">ICLR</strong>), Kigali Rwanda, 2023. </i>
        <a href="https://arxiv.org/abs/2210.00643" target="_blank">[Paper]</a>
        <a href="https://github.com/Louise-LuLin/GCL-SPAN" target="_blank">[Code]</a>
        </p>
        <!-- <p style="color:blue;">A short version of this paper also appears on NeurIPS 2022 Workshop on New Frontiers in Graph Learning. </p> -->         
      </li>

<!--       <li class="paper">
        <b>Robustness for Free: Adversarially Robust Anomaly Detection Through Diffusion Model</b></br>
        <p><u>Yuanpu Cao</u>, Lu Lin, and <b>Jinghui Chen</b>, <i> Technical Report. </i>
        <a href="https://openreview.net/forum?id=imIlOpuEsi" target="_blank">[Paper]</a>
        </p>
      </li> -->


      <li class="paper">
        <b>On the Vulnerability of Backdoor Defenses for Federated Learning</b></br> 
        <p><u>Pei Fang</u> and <b>Jinghui Chen</b>, 
        <i>  in Proceedings of the 37th Conference on Artificial Intelligence (<strong style="color:red;">AAAI</strong>), Washington DC, USA, 2023. </i>
        <a href="https://arxiv.org/abs/2301.08170" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/Focused-Flip-Federated-Backdoor-Attack" target="_blank">[Code]</a>
        </p>
          <!-- <p style="color:blue;">A short version of this paper also appears on International Workshop on Federated Learning: Recent Advances and New Challenges in Conjunction with NeurIPS 2022. </p> -->
        </li>
 
      <li class="paper">
        <b>One-shot Neural Backdoor Erasing via Adversarial Weight Masking</b></br>
        <p><u>Shuwen Chai</u>  and <b>Jinghui Chen</b>, <i> in Proceedings of the 36th Advances in Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, LA, USA, 2022. </i>
        <a href="https://arxiv.org/abs/2207.04497" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/AWM" target="_blank">[Code]</a>
        </p>
      </li>

      <li class="paper">
        <b>Accelerating Adaptive Federated Optimization with Local Gossip Communications</b></br>
        <p><u>Yujia Wang</u>, <u>Pei Fang</u> and <b>Jinghui Chen</b>, <i> in International Workshop on Federated Learning: Recent Advances and New Challenges in Conjunction with NeurIPS 2022 (<strong style="color:red;">FL-NeurIPS</strong>), 2022. </i>
        <a href="https://openreview.net/forum?id=wwXb1qmcBuD" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>How Powerful is Implicit Denoising in Graph Neural Networks</b></br>
        <p>Songtao Liu, Zhitao Ying, Hanze Dong, Lu Lin, <b>Jinghui Chen</b> and Dinghao Wu, <i>NeurIPS 2022 Workshop on New Frontiers in Graph Learning (<strong style="color:red;">GLFrontiers-NeurIPS</strong>). </i>
        <a href="https://arxiv.org/abs/2209.14514" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>The United States COVID-19 Forecast Hub dataset</b></br>
        <p>Estee Y Cramer, ..., <b>Jinghui Chen</b>, ..., Nicholas G. Reich, <i><strong style="color:red;">Scientific Data</strong>, 9(1), pp.1-15., 2022. </i>
        <a href="https://www.nature.com/articles/s41597-022-01517-w#" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>LeapAttack: Hard-Label Adversarial Attack on Text via Gradient-Based Optimization</b></br>
        <p>Muchao Ye, <b>Jinghui Chen</b>, Chenglin Miao, Ting Wang and Fenglong Ma, <i> in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<strong style="color:red;">KDD</strong>), Washington DC, USA, 2022. </i>
        <a href="https://dl.acm.org/doi/abs/10.1145/3534678.3539357" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>Communication-Efficient Adaptive Federated Learning</b></br>
        <p><u>Yujia Wang</u>, Lu Lin and <b>Jinghui Chen</b>, <i> in Proceedings of the 39th International Conference on Machine Learning (<strong style="color:red;">ICML</strong>), Baltimore, Maryland, USA, 2022. </i>
        <a href="https://arxiv.org/abs/2205.02719" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/FedCAMS" target="_blank">[Code]</a>
        </p> 
      </li> 
    
      <li class="paper">
        <b>Evaluation of inliidual and ensemble probabilistic forecasts of COVID-19 mortality in the US</b></br>
        <p>Estee Y Cramer, ..., <b>Jinghui Chen</b>, ..., Nicholas G. Reich, <i>in Proceedings of the National Academy of Sciences (<strong style="color:red;">PNAS</strong>), 2022.</i>
        <a href="https://www.pnas.org/doi/full/10.1073/pnas.2113561119" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations</b></br>
        <p><u>Weiqi Peng</u> and <b>Jinghui Chen</b>, <i>in Proceedings of the 10th International Conference on Learning Representations (<strong style="color:red;">ICLR</strong>), Virtual, 2022.</i>
        <a href="https://openreview.net/forum?id=6VpeS27viTq" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/Learnability-Lock" target="_blank">[Code]</a>
        </p>
      </li>

      <li class="paper">
        <b>Communication-Compressed Adaptive Gradient Method for Distributed Nonconvex Optimization</b></br>
        <p><u>Yujia Wang</u>, Lu Lin and <b>Jinghui Chen</b>, <i>in Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (<strong style="color:red;">AISTATS</strong>), Virtual, 2022. </i>
        <a href="https://arxiv.org/abs/2111.00705" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/CD-Adam" target="_blank">[Code]</a>
        </p>
        <!-- <p style="color:blue;">A short version of this paper also appears on International Workshop on Trustable, Verifiable and Auditable Federated Learning in Conjunction with AAAI 2022 (FL-AAAI-22). </p> -->
      </li>

      <li class="paper">
        <b>Efficient Robust Training via Backward Smoothing</b></br>
        <p><b>Jinghui Chen</b>, Yu Cheng, Zhe Gan, Quanquan Gu and Jingjing Liu, <i>In Proceedings of the 36th AAAI Conference on Artificial Intelligence (<strong style="color:red;">AAAI</strong>), Vancouver, BC, Canada, 2022.</i>
        <a href="https://arxiv.org/abs/2010.01278" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/BackwardSmoothing" target="_blank">[Code]</a>
        </p>
      </li>
   
      <li class="paper">
        <b>Do Wider Neural Networks Really Help Adversarial Robustness?</b></br>
        <p>Boxi Wu<sup><b>E</b></sup>, <b>Jinghui Chen<sup><b>E</b></sup></b>, Deng Cai, Xiaofei He and Quanquan Gu, <i>in Proceedings of the 35th Advances in Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), Virtual, 2021.</i>
        <a href="https://arxiv.org/abs/2010.01279" target="_blank">[Paper]</a>
        </p>
      </li>

     <li class="paper">
        <b>Epidemic Model Guided Machine Learning for COVID-19 Forecasts in the United States</b></br>
        <p>Difan Zou, Lingxiao Wang, Pan Xu, <b>Jinghui Chen</b>, Weitong Zhang and Quanquan Gu, <i>ICLR 2021 Workshop on Machine Learning for Preventingand Combating Pandemics (<strong style="color:red;">MLPCP-ICLR</strong>).</i>
        <a href="https://www.medrxiv.org/content/10.1101/2020.05.24.20111989" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization</b></br>
        <p>Dongruo Zhou<sup><b>E</b></sup>, <b>Jinghui Chen<sup><b>E</b></sup></b>, Yuan Cao<sup><b>E</b></sup>, Yiqi Tang, Ziyan Yang, and Quanquan Gu, <i>NeurIPS 2020 Workshop on Optimization for Machine Learning (<strong style="color:red;">OPT-NeurIPS</strong>).</i>
        <a href="https://arxiv.org/abs/1808.05671" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>Ensemble Forecasts of Coronavirus Disease 2019 (COVID-19) in the U.S</b></br>
        <p>COVID-19 Forecast Hub Consortium, <b>Jinghui Chen</b>., <i>medRxiv:2020.08.19.20177493, 2020.</i>
        <a href="https://www.medrxiv.org/content/10.1101/2020.08.19.20177493" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>RayS: A Ray Searching Method for Hard-label Adversarial Attack</b></br>
        <p><b>Jinghui Chen</b> and Quanquan Gu, <i>in Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<strong style="color:red;">KDD</strong>), 
          San Diego, CA, USA 2020. </i>
        <a href="https://arxiv.org/abs/2006.12792" target="_blank">[Paper]</a>
        <a href="https://github.com/uclaml/RayS" target="_blank">[Code]</a>
        </p>
          <!-- <p style="color:blue;">A short version of this paper also appears on ICML 2020 Workshop on Uncertainty & Robustness in Deep Learning and ECCV 2020 Workshop on Adversarial Robustness in the Real World.  </p> -->
      </li>

      <li class="paper">
        <b>Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks</b></br>
        <p><b>Jinghui Chen</b>, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao and Quanquan Gu, <i>in Proceedings of 29th International Joint Conference on Artificial Intelligence (<strong style="color:red;">IJCAI</strong>),
          Yokohama, Japan, 2020. </i>
        <a href="https://arxiv.org/abs/1806.06763" target="_blank">[Paper]</a>
        <a href="https://github.com/uclaml/Padam" target="_blank">[Code]</a>
        </p>
      </li>

      <li class="paper">
        <b>Understanding the Intrinsic Robustness of Image Distributions using Conditional Generative Models</b></br>
        <p>Xiao Zhang<sup><b>E</b></sup>, <b>Jinghui Chen<sup><b>E</b></sup></b>, Quanquan Gu and David Evans, <i>in Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (<strong style="color:red;">AISTATS</strong>),
          Palermo, Sicily, Italy, 2020.  </i>
        <a href="https://arxiv.org/abs/2003.00378" target="_blank">[Paper]</a>
        <a href="https://github.com/xiaozhanguva/Intrinsic-Rob" target="_blank">[Code]</a>
        </p>
      </li>

      <li class="paper">
        <b>A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks </b></br>
        <p><b>Jinghui Chen</b>, Dongruo Zhou, Jinfeng Yi and Quanquan Gu, <i>in Proceedings of the 34th Conference on Artificial Intelligence (<strong style="color:red;">AAAI</strong>), New York, New York, USA, 2020.  </i>
        <a href="https://arxiv.org/abs/1811.10828" target="_blank">[Paper]</a>
        <a href="https://github.com/uclaml/Frank-Wolfe-AdvML" target="_blank">[Code]</a>
        </p>
      </li>

      <li class="paper">
        <b>Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization</b></br>
        <p>Pan Xu<sup><b>E</b></sup>, <b>Jinghui Chen<sup><b>E</b></sup></b>, Difan Zou and Quanquan Gu, <i>in Proceedings of the 32nd Advances in Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), Montréal, Canada, 2018. 
         <!-- <font color="red"> (Spotlight, 3.5%) </font> -->
         </i>
        <a href="https://arxiv.org/abs/1707.06618" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>Covariate Adjusted Precision Matrix Estimation via Nonconvex Optimization</b></br>
        <p><b>Jinghui Chen</b>, Pan Xu, Lingxiao Wang, Jian Ma and Quanquan Gu, <i>in Proceedings of the 35th International Conference on Machine Learning (<strong style="color:red;">ICML</strong>), Stockholm, Sweden, 2018. 
          <!-- <font color="red">  (Long Oral Presentation, 4.8%)</font> -->
        </i>
        <a href="http://proceedings.mlr.press/v80/chen18n.html" target="_blank">[Paper]</a>
        <a href="https://github.com/uclaml/nonconvex-CGGM" target="_blank">[Code]</a>
        </p>
      </li>

      <li class="paper">
        <b>Fast Newton Hard Thresholding Pursuit for Sparsity Constrained Nonconvex Optimization</b></br>
        <p><b>Jinghui Chen</b> and Quanquan Gu, <i>in Proceedings of the 23rd ACM SIGKDD Conference
          on Knowledge Discovery and Data Mining (<strong style="color:red;">KDD</strong>), Halifax, Nova Scotia, Canada, 2017.</i>
        <a href="https://www.kdd.org/kdd2017/papers/view/fast-newton-hard-thresholding-pursuit-for-sparsity-constrained-nonconvex-op" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>Robust Wirtinger Flow for Phase Retrieval with Arbitrary Corruption</b></br>
        <p><b>Jinghui Chen</b>, Lingxiao Wang, Xiao Zhang, and Quanquan Gu, <i>arXiv:1704.06256, 2017.</i>
        <a href="https://arxiv.org/abs/1704.06256" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>Outlier Detection with Autoencoder Ensembles</b></br>
        <p><b>Jinghui Chen</b>, Saket Sathe, Charu Aggarwal, and Deepak Turaga, <i>in Proceedings of the 17th SIAM International Conference on Data Mining (<strong style="color:red;">SDM</strong>), Houston, Texas, USA, 2017. </i>
        <a href="https://saketsathe.net/downloads/autoencode.pdf" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>Stochastic Block Coordinate Gradient Descent for Sparsity Constrained Optimization</b></br>
        <p><b>Jinghui Chen</b> and Quanquan Gu, <i>in Proceedings of the 32th International Conference on Uncertainty in Artificial Intelligence (<strong style="color:red;">UAI</strong>), New York, USA, 2016. </i>
        <a href="http://auai.org/uai2016/proceedings/papers/202.pdf" target="_blank">[Paper]</a> 
        </p>
      </li>

      <li class="paper">
        <b>Improved threshold Selection by using Calibrated Probabilities for Random Forest Classifiers</b></br>
        <p>Florian Baumann, <b>Jinghui Chen</b>, Karsten Vogt and Bodo Rosenhahn, <i>in Proceedings of the 12th Conference on Computer and Robot Vision (<strong style="color:red;">CRV</strong>), Halifax, Nova Scotia, Canada,
          2015.  </i>
        <a href="https://ieeexplore.ieee.org/document/7158334" target="_blank">[Paper]</a> 
        </p>
      </li>
    </ol>
        

    <ol class="tab-pane" id="at">

      <li class="paper">
        <b>VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models</b> 
        <br>
        <p>Ziyi Yin, Muchao Ye, <u>Tianrong Zhang</u>, Jiaqi Wang, Han Liu, <b>Jinghui Chen</b>, Ting Wang and Fenglong Ma, <i>in Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence (<strong style="color:red;">AAAI</strong>), Vancouver, Canada, 2024. </i>
        <!-- <a href=" " target="_blank">[Paper]</a> -->
        </p>
       </li>

      <li class="paper">
        <b>On Defending Contrastive Learning against Backdoor Attacks</b> 
        <br>
        <p>Changjiang Li, Ren Pang, <u>Bochuan Cao</u>, Zhaohan Xi, <b>Jinghui Chen</b>, Shouling Ji and Ting Wang, <i>in Proceedings of the 33rd USENIX Security Symposium (<strong style="color:red;">USENIX</strong>), Philadelphia, PA, USA, 2024.</i>
        <!-- <a href=" " target="_blank">[Paper]</a> -->
        </p>
       </li>


      <li class="paper">
        <b>Unveiling the Secrets without Data: Can Graph Neural Networks Be Exploited through Data-Free Model Extraction Attacks?</b> 
        <br>
        <p>Yuanxin Zhuang, Chuan Shi, Mengmei Zhang, <b>Jinghui Chen</b>, Lingjuan Lyu, Pan Zhou and Lichao Sun,  <i>in Proceedings of the 33rd USENIX Security Symposium (<strong style="color:red;">USENIX</strong>), Philadelphia, PA, USA, 2024.</i>
        <!-- <a href=" " target="_blank">[Paper]</a> -->
        </p>
       </li>

      <li class="paper">
        <b>Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections</b> 
        <br>
        <p><u>Yuanpu Cao</u>, <u>Bochuan Cao</u> and <b>Jinghui Chen</b>, <i>arXiv:2312.00027, 2023.</i>
        <a href="https://arxiv.org/abs/2312.00027" target="_blank">[Paper]</a>
        </p>
       </li>

      <li class="paper">
        <b>On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?</b> 
        <br>
        <p>Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, <u>Bochuan Cao</u>, Lu Lin, Jinyuan Jia, <b>Jinghui Chen</b> and Dinghao Wu, <i>arXiv:2310.01581, 2023. </i>
        <a href="https://arxiv.org/abs/2310.01581" target="_blank">[Paper]</a>
        </p>
       </li>

       <li class="paper">
        <b>Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM</b> 
        <br>
        <p><u>Bochuan Cao</u><sup><b>E</b></sup>, <u>Yuanpu Cao</u><sup><b>E</b></sup>,  Lu Lin, and <b>Jinghui Chen</b>, <i>arXiv:2309.14348, 2023. </i>
        <a href="https://arxiv.org/abs/2309.14348" target="_blank">[Paper]</a>
        </p>
       </li>

      <li class="paper">
        <b>IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI</b> 
        <br>
        <p><u>Bochuan Cao</u>, Changjiang Li, Ting Wang, Jinyuan Jia, Bo Li and <b>Jinghui Chen</b>, <i>in Proceedings of the 37th Conference on Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, Louisiana, USA, 2023. </i>
        <a href="https://arxiv.org/abs/2310.19248" target="_blank">[Paper]</a>
        </p>
       </li>

      <li class="paper">
        <b>VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models</b><br>
        <p>Ziyi Yin, Muchao Ye, <u>Tianrong Zhang</u>, Tianyu Du, Jinguo Zhu, Han Liu, <b>Jinghui Chen</b>, Ting Wang and Fenglong Ma, <i>in Proceedings of the 37th Conference on Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, Louisiana, USA, 2023. </i>
        <a href="https://arxiv.org/abs/2310.04655" target="_blank">[Paper]</a> 
        </p>
      </li>
 
      <li class="paper">
        <b>A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning</b></br>
        <p>Hangfan Zhang, Jinyuan Jia, <b>Jinghui Chen</b>, Lu Lin and Dinghao Wu, <i>in Proceedings of the 37th Conference on Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, Louisiana, USA, 2023. </i>
        <a href="https://openreview.net/forum?id=S6ajVZy6FA" target="_blank">[Paper]</a> 
        </p>
      </li>

      <li class="paper">
        <b>Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks</b></br>
        <p>Tianyu Du, Zhaohan Xi, Changjiang Li, Ren Pang, Shouling Ji, <b>Jinghui Chen</b>, Fenglong Ma and Ting Wang, <i>in Proceedings of the 37th Conference on Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, Louisiana, USA, 2023. </i>
        <a href="https://arxiv.org/abs/2309.13256" target="_blank">[Paper]</a> 
        </p>
      </li>

      <li class="paper">
        <b>UniT: A Unified Look at Certified Robust Training against Text Adversarial Perturbation</b></br>
        <p>Muchao Ye, Ziyi Yin, <u>Tianrong Zhang</u>, Tianyu Du, <b>Jinghui Chen</b>, Ting Wang and Fenglong Ma, <i>in Proceedings of the 37th Conference on Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, Louisiana, USA, 2023. </i>
        <a href="https://openreview.net/forum?id=oGxE2Nvlda" target="_blank">[Paper]</a> 
        </p>
      </li>

      <li class="paper">
        <b>RoCourseNet: Robust Training of a Prediction Aware Recourse Model</b></br>
        <p>Hangzhi Guo, Feiran Jia, <b>Jinghui Chen</b>, Anna Squicciarini and Amulya Yadav, <i>in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (<strong style="color:red;">CIKM</strong>), Birmingham, UK, 2023. </i>
        <a href="https://arxiv.org/abs/2206.00700" target="_blank">[Paper]</a> 
        </p>
    <!--     <p style="color:blue;">A short version of this paper also appears on ICML 2022 Workshop on Adversarial Machine Learning Frontiers. </p>
           <li class="paper-buttons">
              <a class="button" href="https://arxiv.org/abs/2206.00700" target="_blank">Paper</a>
          </li> -->
      </li>


      <li class="paper">
        <b>PAT: Geometry-Aware Hard-Label Black-Box Adversarial Attacks on Text</b></br>
        <p>Muchao Ye, <b>Jinghui Chen</b>, Chenglin Miao, Han Liu, Ting Wang and Fenglong Ma, <i> in Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<strong style="color:red;">KDD</strong>), San Diego, CA, USA, 2023.</i>
        <a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599461" target="_blank">[Paper]</a>
        </p>           
      </li>

      <li class="paper">
        <b>Benign Overfitting in Adversarially Robust Linear Classification</b></br>
        <p><b>Jinghui Chen<sup><b>E</b></sup></b>, Yuan Cao<sup><b>E</b></sup>, and Quanquan Gu, <i> in Proceedings of the 39th Conference on Uncertainty in Artificial Intelligence (<strong style="color:red;">UAI</strong>), Pittsburgh, PA, USA, 2023.</i> <a href="https://arxiv.org/abs/2112.15250" target="_blank">[Paper]</a> </p>
         <!-- <p style="color:blue;">A short version of this paper also appears on ICML 2021 Workshop on Overparameterization: Pitfalls and Opportunities.</p> -->
      </li>

      <li class="paper">
        <b>Graph Contrastive Backdoor Attacks</b></br>
        <p>Hangfan Zhang, <b>Jinghui Chen</b>, Lu Lin, Jinyuan Jia and Dinghao Wu, <i> in Proceedings of the 40th International Conference on Machine Learning (<strong style="color:red;">ICML</strong>), Hawaii, USA, 2023. </i> 
        <a href="https://proceedings.mlr.press/v202/zhang23e/zhang23e.pdf" target="_blank">[Paper]</a> </p>
      </li>


      <li class="paper">
        <b>Tackling the Data Heterogeneity in Asynchronous Federated Learning with Cached Update Calibration</b></br>
        <p><u>Yujia Wang</u>, <u>Yuanpu Cao</u>, <u>Jingcheng Wu</u>, <u>Ruoyu Chen</u> and <b>Jinghui Chen</b>, <i> in ICML 2023 Workshop on Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities (<strong style="color:red;">FL-ICML</strong>), 2023. </i> 
        <a href="https://openreview.net/forum?id=WpjdIMouJj" target="_blank">[Paper]</a> </p>
      </li>

 

      <li class="paper">
        <b>Do Language Models Plagiarize?</b></br>
        <p>Lee, Jooyoung, Thai Le, <b>Jinghui Chen</b>, and Dongwon Lee, <i>in Proceedings of the ACM Web Conference (<strong style="color:red;">WWW</strong>), Austin, Texas, USA, 2023. </i>
        <a href="https://arxiv.org/abs/2203.07618" target="_blank">[Paper]</a> </p>    
      </li>


      <li class="paper">
        <b>Spectral Augmentation for Self-Supervised Learning on Graphs</b></br>
        <p>Lu Lin, <b>Jinghui Chen</b>, Hongning Wang, <i> in Proceedings of the 11th International Conference on Learning Representations (<strong style="color:red;">ICLR</strong>), Kigali Rwanda, 2023. </i>
        <a href="https://arxiv.org/abs/2210.00643" target="_blank">[Paper]</a>
        <a href="https://github.com/Louise-LuLin/GCL-SPAN" target="_blank">[Code]</a>
        </p>
        <!-- <p style="color:blue;">A short version of this paper also appears on NeurIPS 2022 Workshop on New Frontiers in Graph Learning. </p> -->         
      </li>

<!--       <li class="paper">
        <b>Robustness for Free: Adversarially Robust Anomaly Detection Through Diffusion Model</b></br>
        <p><u>Yuanpu Cao</u>, Lu Lin, and <b>Jinghui Chen</b>, <i> Technical Report. </i>
        <a href="https://openreview.net/forum?id=imIlOpuEsi" target="_blank">[Paper]</a>
        </p>
      </li> -->


      <li class="paper">
        <b>On the Vulnerability of Backdoor Defenses for Federated Learning</b></br> 
        <p><u>Pei Fang</u> and <b>Jinghui Chen</b>, 
        <i>  in Proceedings of the 37th Conference on Artificial Intelligence (<strong style="color:red;">AAAI</strong>), Washington DC, USA, 2023. </i>
        <a href="https://arxiv.org/abs/2301.08170" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/Focused-Flip-Federated-Backdoor-Attack" target="_blank">[Code]</a>
        </p>
          <!-- <p style="color:blue;">A short version of this paper also appears on International Workshop on Federated Learning: Recent Advances and New Challenges in Conjunction with NeurIPS 2022. </p> -->
        </li>
 
      <li class="paper">
        <b>One-shot Neural Backdoor Erasing via Adversarial Weight Masking</b></br>
        <p><u>Shuwen Chai</u>  and <b>Jinghui Chen</b>, <i> in Proceedings of the 36th Advances in Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), New Orleans, LA, USA, 2022. </i>
        <a href="https://arxiv.org/abs/2207.04497" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/AWM" target="_blank">[Code]</a>
        </p>
      </li>

      <li class="paper">
        <b>Accelerating Adaptive Federated Optimization with Local Gossip Communications</b></br>
        <p><u>Yujia Wang</u>, <u>Pei Fang</u> and <b>Jinghui Chen</b>, <i> in International Workshop on Federated Learning: Recent Advances and New Challenges in Conjunction with NeurIPS 2022 (<strong style="color:red;">FL-NeurIPS</strong>), 2022. </i>
        <a href="https://openreview.net/forum?id=wwXb1qmcBuD" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>How Powerful is Implicit Denoising in Graph Neural Networks</b></br>
        <p>Songtao Liu, Zhitao Ying, Hanze Dong, Lu Lin, <b>Jinghui Chen</b> and Dinghao Wu, <i>NeurIPS 2022 Workshop on New Frontiers in Graph Learning (<strong style="color:red;">GLFrontiers-NeurIPS</strong>). </i>
        <a href="https://arxiv.org/abs/2209.14514" target="_blank">[Paper]</a>
        </p>
      </li>
 

      <li class="paper">
        <b>LeapAttack: Hard-Label Adversarial Attack on Text via Gradient-Based Optimization</b></br>
        <p>Muchao Ye, <b>Jinghui Chen</b>, Chenglin Miao, Ting Wang and Fenglong Ma, <i> in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<strong style="color:red;">KDD</strong>), Washington DC, USA, 2022. </i>
        <a href="https://dl.acm.org/doi/abs/10.1145/3534678.3539357" target="_blank">[Paper]</a>
        </p>
      </li>

      <li class="paper">
        <b>Communication-Efficient Adaptive Federated Learning</b></br>
        <p><u>Yujia Wang</u>, Lu Lin and <b>Jinghui Chen</b>, <i> in Proceedings of the 39th International Conference on Machine Learning (<strong style="color:red;">ICML</strong>), Baltimore, Maryland, USA, 2022. </i>
        <a href="https://arxiv.org/abs/2205.02719" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/FedCAMS" target="_blank">[Code]</a>
        </p> 
      </li> 
 

      <li class="paper">
        <b>Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations</b></br>
        <p><u>Weiqi Peng</u> and <b>Jinghui Chen</b>, <i>in Proceedings of the 10th International Conference on Learning Representations (<strong style="color:red;">ICLR</strong>), Virtual, 2022.</i>
        <a href="https://openreview.net/forum?id=6VpeS27viTq" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/Learnability-Lock" target="_blank">[Code]</a>
        </p>
      </li>

      <li class="paper">
        <b>Communication-Compressed Adaptive Gradient Method for Distributed Nonconvex Optimization</b></br>
        <p><u>Yujia Wang</u>, Lu Lin and <b>Jinghui Chen</b>, <i>in Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (<strong style="color:red;">AISTATS</strong>), Virtual, 2022. </i>
        <a href="https://arxiv.org/abs/2111.00705" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/CD-Adam" target="_blank">[Code]</a>
        </p>
        <!-- <p style="color:blue;">A short version of this paper also appears on International Workshop on Trustable, Verifiable and Auditable Federated Learning in Conjunction with AAAI 2022 (FL-AAAI-22). </p> -->
      </li>

      <li class="paper">
        <b>Efficient Robust Training via Backward Smoothing</b></br>
        <p><b>Jinghui Chen</b>, Yu Cheng, Zhe Gan, Quanquan Gu and Jingjing Liu, <i>In Proceedings of the 36th AAAI Conference on Artificial Intelligence (<strong style="color:red;">AAAI</strong>), Vancouver, BC, Canada, 2022.</i>
        <a href="https://arxiv.org/abs/2010.01278" target="_blank">[Paper]</a>
        <a href="https://github.com/jinghuichen/BackwardSmoothing" target="_blank">[Code]</a>
        </p>
      </li>
   
      <li class="paper">
        <b>Do Wider Neural Networks Really Help Adversarial Robustness?</b></br>
        <p>Boxi Wu<sup><b>E</b></sup>, <b>Jinghui Chen<sup><b>E</b></sup></b>, Deng Cai, Xiaofei He and Quanquan Gu, <i>in Proceedings of the 35th Advances in Neural Information Processing Systems (<strong style="color:red;">NeurIPS</strong>), Virtual, 2021.</i>
        <a href="https://arxiv.org/abs/2010.01279" target="_blank">[Paper]</a>
        </p>
      </li>
       
      
    </ol>

  </div>
</div>


<!-- ========== Research ========== -->
<div class="docs-section" id="research">
  <h4>Research</h4>

  <p>
The research of our lab is focused on different aspects of machine learning (efficiency, robustness, interpretability, responsibility, trustworthiness) and their applications in computer vision, graph learning, anomaly detection, cybersecurity, recommendation systems, computational genomics, etc. Some of our current research projects are:
  </p>
  <div class="row">
        
 
        <!-- <article class="major">
          <Li> Adversarial Machine Learning </Li>
          <Li> Nonconvex Optimization in Machine Learning </Li>
            <Li> Evasion/Poisoning Attacks and Defenses on Machine Learning Systems </Li>
            <Li> Federated and Distributed Machine Learning </Li>
            <Li> Explainable/Interpretable Machine Learning </Li>
            <Li> Security and Privacy Issues in Machine Learning </Li>
        </article>-->


      <div class="six columns">
        
        <div class="project-container">
            <div class="project-image-container">    
                <img src="assets/research-pics/llm.png" class="u-full-width-height"  height="115" />
            </div>
            <div class="project-caption">
              <b>Trustworthiness and Saftey Issues in Large Language Models </b> <br/>
              Studying the vulnerabilities inside the current LLMs and how we can improve them for better trustworthiness.
            </div>
        </div>

        

        
        <div class="project-container">
            <div class="project-image-container">    
                <img src="assets/research-pics/opt.png" class="u-full-width-height" />
            </div>
            <div class="project-caption">
              <b>Optimization in Machine Learning</b> <br/>
              Studying the convergence of machine learning optimizers including adaptive gradient optimizers and designing new generation of optimizers for deep learning.
            </div>
        </div>

        <div class="project-container">
            <div class="project-image-container">    
                <img src="assets/research-pics/backdoor.png" class="u-full-width-height" />
            </div>
            <div class="project-caption">
              <b>Poisoning/Backdoor Attacks and Defenses  </b> <br/>
              Studying the effects of poisoning and backdoor attacks on deep learning models, as well as how to mitigate those threats.
            </div>
        </div>

        
        
      </div>
        
      <div class="six columns">
        <div class="project-container">
            <div class="project-image-container">    
                <img src="assets/research-pics/fedml.png" class="u-full-width-height" />
            </div>
            <div class="project-caption">
              <b>Federated Machine Learning</b> <br/>
              Addressing the emerging challenges for Federated Learning in practical scenarios such as data and model heterogeneity, communication efficiency, as well as security and privacy issues.
            </div>
        </div>

        <div class="project-container">
           <div class="project-image-container">    
                <img src="assets/research-pics/advml.png" class="u-full-width-height" />
            </div>
            <div class="project-caption">
              <b>Adversarial Robustness in Machine Learning</b> <br/>
              Evaluating, understanding, and improving adversarial robustness in deep learning as well as studying the theoretical foundations behind adversarial training and robust learning.
            </div>
        </div>

        
        <div class="project-container">
            <div class="project-image-container">    
                <img src="assets/research-pics/gnn.png" class="u-full-width-height" />
            </div>
            <div class="project-caption">
              <b>Robustness in Graph Nerual Networks</b> <br/>
              Improving the robustness of the current graph neural networks again graph structural/feature perturbations.
            </div>
        </div>

        <!-- <div class="project-container">
            <div class="project-image-container">    
                <img src="assets/research-pics/learnability.png" class="u-full-width-height" />
            </div>
            <div class="project-caption">
              <b>Learnability Attacks on Machine Learning Systems </b> <br/>
              Studying how to manipulate data to make machine learning model unlearnable.
            </div>
        </div> -->

        
      </div>

      

        
  </div>

</div>

 
 

<!-- ========== Students ========== -->
<div class="docs-section" id="students">
  <h4>Students</h4>

  <!-- <ul class="tab-nav">
    <li><div class="button active" data-ref="#phd">Ph.D.</div></li>
    <li><div class="button" data-ref="#master">Master</div></li>
    <li><div class="button" data-ref="#undergrad">Undergrad</div></li>    
  </ul>

  <div class="tab-content">
    <div class="tab-pane active" id="phd">
      
      
        <div class="student">
          
          <p class="title"><b><a href=''>Yujia Wang</a></b> (Spring 2022 -  )</p> 
          
        </div>   
      
    </div>

    <div class="tab-pane" id="master">
      
      
    </div>

    <div class="tab-pane" id="undergrad">
      
      
    </div>    

  </div> -->

  <div class="student">  

  <b>Current Ph.D. Students</b> 
  <ul>    
      <li><b> <a>Yujia Wang</a></b> (Ph.D., Spring 2022 - Present) </b>
      <li><b> <a>Bochuan Cao</a></b> (Ph.D., Fall 2022 - Present) </b>
      <li><b> <a>Tianrong Zhang</a></b> (Ph.D. co-advising with Dr. Prasenjit Mitra, Fall 2022 - Present) </b>
      <li><b> <a>Yuanpu Cao</a></b> (Ph.D., Fall 2023 -  Present) </b> 
  </ul>

  <b>Current Undergrad/Master/Intern Students</b> 
  <ul>    
      <li><b> <a>Ben Hsiao</a></b> (Undergrad at PSU, Fall 2023 -  Present) </b>
      <li><b> <a>Malcolm Zerbe</a></b> (Undergrad at PSU, Fall 2023 -  Present) </b>
      <li><b> <a>Bharavi Misra</a></b> (Undergrad at PSU, Fall 2023 -  Present) </b>
  </ul>

   <b>Alumni</b> 
   <ul>   
      <li><b> <a>Avi Bewtra</a></b> (Undergrad at PSU, Fall 2021 -  Spring 2022) </b>
      <li><b> <a>Weiqi Peng</a></b> (Research Intern, Fall 2021 - Spring 2022, Now at Amazon) </b>
      <li><b> <a>Shuwen Chai</a></b> (Research Intern, Fall 2021 -  Spring 2022, Now a Ph.D. student at Northwestern University) </b>
      <li><b> <a>Pei Fang</a></b> (Research Intern, Fall 2021 -  Fall 2023, Now at Ant Financial) </b>
      <li><b> <a>Tiejin Chen</a></b> (Research Intern, Summer 2022 -  Fall 2022, Now a Ph.D. student at ASU) </b>
      <li><b> <a>Aryan Harshanan Patil</a></b> (Undergrad at PSU, Fall 2022 -  Spring 2023) </b>
      <li><b> <a>Sirui Qi</a></b> (Undergrad at PSU, Fall 2022 -  Spring 2023) </b>
      <li><b> <a>Sooraj Narayanan Sekar</a></b> (Undergrad at PSU, Fall 2022 -  Spring 2023)  </b>
      <li><b> <a>Jingcheng Wu</a></b> (Research Intern, Spring 2023 -  Fall 2023, Now at Google) </b>
      <li><b> <a>Ruoyu Chen</a></b> (Research Intern, Spring 2023 -  Fall 2023) </b>

  </ul>
  </div>   

</div>


<!-- ========== TEACHING ========== -->
<span class="anchor" id="teaching"></span>
<div class="docs-section">
  <h4>Teaching</h4>

 

  <p>
  <ul>
  <li><b> Fall 2023: <a href=https://jinghuichen.github.io/DS310-23Fall/>DS310: Machine Learning for Data Analytics</a>  </b>
  <li><b> Spring 2023: <a href=https://jinghuichen.github.io/SRA221-23Spring/>SRA221: Overview of Information Security</a>  </b>
  <li><b> Fall 2022: <a href=https://jinghuichen.github.io/AdvML22Fall/>IST597: Special Topics on Adversarial Machine Learning</a>  </b>
  <li><b> Spring 2022: <a href=https://jinghuichen.github.io/SRA221-22Spring/>SRA221: Overview of Information Security</a>  </b>
  <li><b> Fall 2021: <a href=https://jinghuichen.github.io/AdvML21Fall/>IST597: Special Topics on Adversarial Machine Learning</a>  </b>

  </ul>

  </p>

</div>


<!-- ========== Service ========== -->
<span class="anchor" id="service"></span>
<div class="docs-section">
  <h4>Acadamic Service</h4>

      
    <b>Senior Program Committee</b> 

    <ul>
      <Li> International Joint Conference on Artificial Intelligence (IJCAI)</Li>
      <Li> AAAI Conference on Artificial Intelligence (AAAI)</Li> 
    </ul>

    <b>Program Committee/Reviewer</b> 
    <ul>

      <Li> International Conference on Machine Learning (ICML)</Li>
      <Li> Neural Information Processing Systems (NeurIPS)</Li>
      <Li> International Conference on Learning Representations (ICLR)</Li>
      <Li> The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) </Li>
      <Li> International Conference on Computer Vision (ICCV)</Li>
      <Li> ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</Li>
      <Li> International Conference on Artificial Intelligence and Statistics (AISTATS)</Li>
      <Li> The Conference on Uncertainty in Artificial Intelligence (UAI)</Li>
      <Li> IEEE International Conference on Big Data (BigData)</Li> 
      <Li> SIAM International Conference on Data Mining (SDM) </Li> 
    </ul>

    <b>Journal Reviewer</b> 
    <ul>
      <Li> Journal of Machine Learning Research (JMLR)</Li>
      <Li> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</Li>
      <Li> IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</Li>
      <Li> IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</Li>
      <Li> IEEE Transactions on Knowledge and Data Engineering (TKDE)</Li>
      <Li> IEEE Signal Processing Letters (SPL)</Li>
      <Li> ACM Transactions on Knowledge Discovery from Data (TKDD)</Li>
      <Li> Journal of Artificial Intelligence Research (JAIR)</Li>
      <Li> Journal of Industrial and Management Optimization (JIMO) </Li>
      <Li> Journal of Computational and Applied Mathematics (ELSCAM)</Li>
      <Li> Neural Networks (NEUNET)</Li>
      <Li> BioData Mining (BIDM)</Li>
      <Li> Machine Learning (MACH)</Li>
      <Li> Mathematical Biosciences and Engineering (MBE)</Li>
      <Li> Reviews in Biomedical Engineering (RBME)</Li>
      <Li> Pattern Recognition Letters (PRLETTERS)</Li>
      <Li> Transactions on Big Data (TBD)</Li>
      <Li> Machine Learning and Knowledge Extraction (MAKE)</Li>
      <Li> Advanced Theory and Simulations </Li>
      <Li> Frontiers in Artificial Intelligence</Li>
      <Li> PLOS ONE</Li>
      <Li> PLOS Global Public Health</Li>
      <Li> Applied Science</Li>
      <Li> Algorithms</Li>
      <Li> Electronics</Li>
      <Li> Neurocomputing</Li>
      <Li> Entropy</Li>
    </ul>
 

</div>
 
 


<div class="footer">
  <div class="row">
     <font size="2">© Copyright 2022 Jinghui Chen. Powered by <a href="https://jekyllrb.com">jekyll</a> with theme from <a href="http://web.media.mit.edu/~msaveski/">Martin Saveski</a>. Hosted by GitHub Pages. </font>
  </div>
</div>

 

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
